{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/08/iAH8qYypvLq74MHta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadDastouri/tatoeba_text_analysis/blob/main/tatoeba_text_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn matplotlib seaborn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h-t0JHCjBXS",
        "outputId": "6b346f4c-bb9f-48cd-8ef0-6b908a093d83"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Dict, Tuple\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class TatoebaTextAnalyzer:\n",
        "    \"\"\"Class for analyzing and preprocessing multilingual text data from Tatoeba.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.languages = ['english', 'spanish', 'french', 'german']\n",
        "        self.stopwords = {lang: set(stopwords.words(lang)) for lang in self.languages}\n",
        "\n",
        "    def load_sample_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load sample data similar to Tatoeba format.\"\"\"\n",
        "        data = {\n",
        "            'text': [\n",
        "                'Hello how are you today?',\n",
        "                'Hola cómo estás hoy?',\n",
        "                'Bonjour comment allez-vous aujourd\\'hui?',\n",
        "                'Hallo wie geht es dir heute?'\n",
        "            ],\n",
        "            'language': ['english', 'spanish', 'french', 'german']\n",
        "        }\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def preprocess_text(self, text: str, language: str) -> str:\n",
        "        \"\"\"Preprocess text by removing special characters and stopwords.\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords if language is supported\n",
        "        if language in self.stopwords:\n",
        "            tokens = [word for word in tokens if word not in self.stopwords[language]]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def get_text_statistics(self, text: str) -> Dict:\n",
        "        \"\"\"Calculate basic statistics for a text.\"\"\"\n",
        "        words = word_tokenize(text)\n",
        "        return {\n",
        "            'word_count': len(words),\n",
        "            'char_count': len(text),\n",
        "            'avg_word_length': np.mean([len(word) for word in words]),\n",
        "            'unique_words': len(set(words))\n",
        "        }\n",
        "\n",
        "    def analyze_language_distribution(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Analyze the distribution of languages in the dataset.\"\"\"\n",
        "        return df['language'].value_counts().reset_index()\n",
        "\n",
        "    def get_most_common_words(self, texts: List[str], n: int = 10) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Get the most common words across all texts.\"\"\"\n",
        "        vectorizer = CountVectorizer()\n",
        "        X = vectorizer.fit_transform(texts)\n",
        "        words = vectorizer.get_feature_names_out()\n",
        "        total_counts = X.sum(axis=0).A1\n",
        "        word_freq = list(zip(words, total_counts))\n",
        "        return sorted(word_freq, key=lambda x: x[1], reverse=True)[:n]\n",
        "\n",
        "    def visualize_language_distribution(self, df: pd.DataFrame):\n",
        "        \"\"\"Visualize the distribution of languages.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.countplot(data=df, x='language')\n",
        "        plt.title('Distribution of Languages in Dataset')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_text_lengths(self, df: pd.DataFrame):\n",
        "        \"\"\"Visualize the distribution of text lengths.\"\"\"\n",
        "        df['text_length'] = df['text'].str.len()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(data=df, x='language', y='text_length')\n",
        "        plt.title('Distribution of Text Lengths by Language')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def main():\n",
        "    # Initialize analyzer\n",
        "    analyzer = TatoebaTextAnalyzer()\n",
        "\n",
        "    # Load sample data\n",
        "    df = analyzer.load_sample_data()\n",
        "\n",
        "    # Preprocess texts\n",
        "    df['processed_text'] = df.apply(\n",
        "        lambda row: analyzer.preprocess_text(row['text'], row['language']),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Get text statistics\n",
        "    stats = [analyzer.get_text_statistics(text) for text in df['text']]\n",
        "    stats_df = pd.DataFrame(stats)\n",
        "    print(\"\\nText Statistics:\")\n",
        "    print(stats_df.describe())\n",
        "\n",
        "    # Get language distribution\n",
        "    lang_dist = analyzer.analyze_language_distribution(df)\n",
        "    print(\"\\nLanguage Distribution:\")\n",
        "    print(lang_dist)\n",
        "\n",
        "    # Get most common words\n",
        "    common_words = analyzer.get_most_common_words(df['processed_text'])\n",
        "    print(\"\\nMost Common Words:\")\n",
        "    for word, count in common_words:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    # Visualizations\n",
        "    analyzer.visualize_language_distribution(df)\n",
        "    analyzer.visualize_text_lengths(df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "AnXHkqCljbi5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}